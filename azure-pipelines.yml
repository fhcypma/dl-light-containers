pool:
  vmImage: ubuntu-latest

variables:
  image-name: spark-on-lambda
  tag: '$(Build.BuildNumber)-ci'
  region: eu-west-1


stages:

# - stage: Deploy
#   jobs:
#   - job: Cdk
#     displayName: "Cdk deploy"
#     steps:
#     - task: UsePythonVersion@0
#       inputs:
#         versionSpec: '3.9'
#       displayName: 'Use Python 3.9'

#     - task: NodeTool@0
#       inputs:
#         versionSpec: '14.x'
#       displayName: 'Install 14.x'

#     - script: |
#         sudo npm install -g aws-cdk
#         cdk --version
#       displayName: 'Installing aws cdk'

#     - task: PipAuthenticate@1
#       displayName: 'Pip Authenticate'
#       inputs:
#         artifactFeeds: 'dl-light/dl-light'
#         # Setting this variable to "true" will force pip to get distributions from official python registry first and fallback to feeds mentioned above if distributions are not found there.
#         onlyAddExtraIndex: true

#     # Added a workaround for installing the project feed package
#     - script: |
#         python -m pip install pipenv
#         python -m pip install dl-light-infra --index $(PIP_EXTRA_INDEX_URL)
#         python -m pipenv requirements > requirements.txt
#         python -m pip install -r requirements.txt
#       displayName: "Install dependencies"

#     - task: AWSShellScript@1
#       inputs:
#         awsCredentials: yds-deploy
#         regionName: $(region)
#         scriptType: inline
#         inlineScript: |
#           DYNACONF_BUILD_NUMBER="$(Build.BuildNumber)" cdk deploy --all --require-approval never
#         workingDirectory: $(System.DefaultWorkingDirectory)
#       displayName: "Cdk deploy"

- stage: Publish
  # dependsOn: Deploy
  jobs:
  # - job: Build
  #   displayName: "Spark-on-lambda docker build and push"
  #   steps:

    # - task: Docker@2
    #   displayName: Docker build
    #   inputs:
    #     command: build
    #     Dockerfile: spark-on-lambda/Dockerfile
    #     repository: $(image-name)
    #     tags: $(tag)

    # - task: ECRPushImage@1
    #   displayName: Push to ECR
    #   inputs:
    #     awsCredentials: yds-deploy
    #     regionName: $(region)
    #     imageSource: 'imagename'
    #     sourceImageName: $(image-name)
    #     sourceImageTag: $(tag)
    #     repositoryName: $(image-name)
    #     pushTag: $(tag)

  - job: Test
    steps:

    - task: AWSShellScript@1
      inputs:
        awsCredentials: yds-deploy
        regionName: $(region)
        scriptType: inline
        inlineScript: |
          ls
          zip test.zip main.py
          aws s3 cp test.zip s3://yds.tst.container-test.code
        workingDirectory: $(System.DefaultWorkingDirectory)/spark-on-lambda
      displayName: Deploy test code

    - task: AWSShellScript@1
      inputs:
        awsCredentials: yds-deploy
        regionName: $(region)
        scriptType: inline
        inlineScript: |
          aws lambda invoke \
            --function-name TstContainerTestEtlApplicationsFunction \
            --payload file://test_payload.json \
            --cli-binary-format raw-in-base64-out \
            response.json
        workingDirectory: $(System.DefaultWorkingDirectory)/spark-on-lambda
      displayName: Run lambda

  - job: Release
    steps:

    - task: AWSShellScript@1
      inputs:
        awsCredentials: yds-deploy
        regionName: $(region)
        scriptType: inline
        inlineScript: |
          MANIFEST=$(aws ecr batch-get-image --repository-name $(image-name) --image-ids imageTag=$(tag) --output json | jq --raw-output --join-output '.images[0].imageManifest')
          aws ecr put-image --repository-name $(image-name) --image-tag $(Build.BuildNumber) --image-manifest "$MANIFEST"
          aws ecr batch-delete-image --repository-name "$(image-name)" --image-ids imageTag=$(tag)
        workingDirectory: $(System.DefaultWorkingDirectory)/spark-on-lambda
      displayName: Retag image
